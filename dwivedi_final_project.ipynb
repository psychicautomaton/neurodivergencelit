{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b0fb8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "from simpletransformers.classification import ClassificationModel, ClassificationArgs\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "import datasets\n",
    "from transformers import BertTokenizerFast\n",
    "import transformers\n",
    "import optuna\n",
    "import json\n",
    "import torch\n",
    "import random\n",
    "from transformers import BertForSequenceClassification\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "177f8326",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>ind</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>tok_nums</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>thegoodsoldier</td>\n",
       "      <td>leonora, however, was not in the least keen on...</td>\n",
       "      <td>noble_agented</td>\n",
       "      <td>[101, 6506, 6525, 1010, 2174, 1010, 2001, 2025...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>thegoodsoldier</td>\n",
       "      <td>\"i said a great deal more to him than i wante...</td>\n",
       "      <td>guilt_penance_self_loathing</td>\n",
       "      <td>[101, 1000, 1045, 2056, 1037, 2307, 3066, 2062...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>hamlet</td>\n",
       "      <td>one woe doth tread upon another's heel, so fas...</td>\n",
       "      <td>senseless_meaning_making</td>\n",
       "      <td>[101, 2028, 24185, 2063, 11089, 2232, 29449, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>hamlet</td>\n",
       "      <td>how can that be, unless she drowned herself in...</td>\n",
       "      <td>noble_agented</td>\n",
       "      <td>[101, 2129, 2064, 2008, 2022, 1010, 4983, 2016...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>hamlet</td>\n",
       "      <td>if the man go to this water and drown himself,...</td>\n",
       "      <td>noble_agented</td>\n",
       "      <td>[101, 2065, 1996, 2158, 2175, 2000, 2023, 2300...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>255</td>\n",
       "      <td>255</td>\n",
       "      <td>thewish</td>\n",
       "      <td>\\n\"Thus I sat, as her murderess, at her bedsid...</td>\n",
       "      <td>guilt_penance_self_loathing</td>\n",
       "      <td>[101, 1000, 2947, 1045, 2938, 1010, 2004, 2014...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>256</td>\n",
       "      <td>256</td>\n",
       "      <td>thewish</td>\n",
       "      <td>Mother, I ask you once more: why did she die?\"...</td>\n",
       "      <td>cowardice_foolish_unpleasant</td>\n",
       "      <td>[101, 2388, 1010, 1045, 3198, 2017, 2320, 2062...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257</th>\n",
       "      <td>257</td>\n",
       "      <td>257</td>\n",
       "      <td>thewish</td>\n",
       "      <td>\"'But I know myself,' said she. 'I break it of...</td>\n",
       "      <td>guilt_penance_self_loathing</td>\n",
       "      <td>[101, 1000, 1005, 2021, 1045, 2113, 2870, 1010...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>258</td>\n",
       "      <td>258</td>\n",
       "      <td>thewish</td>\n",
       "      <td>\"What is the good of clinging to happiness whe...</td>\n",
       "      <td>guilt_penance_self_loathing</td>\n",
       "      <td>[101, 1000, 2054, 2003, 1996, 2204, 1997, 1789...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>259</td>\n",
       "      <td>259</td>\n",
       "      <td>thewish</td>\n",
       "      <td>How could I sleep in the bed out of which I wi...</td>\n",
       "      <td>guilt_penance_self_loathing</td>\n",
       "      <td>[101, 2129, 2071, 1045, 3637, 1999, 1996, 2793...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>260 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0  ind           title  \\\n",
       "0             0    0  thegoodsoldier   \n",
       "1             1    1  thegoodsoldier   \n",
       "2             2    2          hamlet   \n",
       "3             3    3          hamlet   \n",
       "4             4    4          hamlet   \n",
       "..          ...  ...             ...   \n",
       "255         255  255         thewish   \n",
       "256         256  256         thewish   \n",
       "257         257  257         thewish   \n",
       "258         258  258         thewish   \n",
       "259         259  259         thewish   \n",
       "\n",
       "                                                  text  \\\n",
       "0    leonora, however, was not in the least keen on...   \n",
       "1     \"i said a great deal more to him than i wante...   \n",
       "2    one woe doth tread upon another's heel, so fas...   \n",
       "3    how can that be, unless she drowned herself in...   \n",
       "4    if the man go to this water and drown himself,...   \n",
       "..                                                 ...   \n",
       "255  \\n\"Thus I sat, as her murderess, at her bedsid...   \n",
       "256  Mother, I ask you once more: why did she die?\"...   \n",
       "257  \"'But I know myself,' said she. 'I break it of...   \n",
       "258  \"What is the good of clinging to happiness whe...   \n",
       "259  How could I sleep in the bed out of which I wi...   \n",
       "\n",
       "                            label  \\\n",
       "0                   noble_agented   \n",
       "1     guilt_penance_self_loathing   \n",
       "2        senseless_meaning_making   \n",
       "3                   noble_agented   \n",
       "4                   noble_agented   \n",
       "..                            ...   \n",
       "255   guilt_penance_self_loathing   \n",
       "256  cowardice_foolish_unpleasant   \n",
       "257   guilt_penance_self_loathing   \n",
       "258   guilt_penance_self_loathing   \n",
       "259   guilt_penance_self_loathing   \n",
       "\n",
       "                                              tok_nums  \n",
       "0    [101, 6506, 6525, 1010, 2174, 1010, 2001, 2025...  \n",
       "1    [101, 1000, 1045, 2056, 1037, 2307, 3066, 2062...  \n",
       "2    [101, 2028, 24185, 2063, 11089, 2232, 29449, 2...  \n",
       "3    [101, 2129, 2064, 2008, 2022, 1010, 4983, 2016...  \n",
       "4    [101, 2065, 1996, 2158, 2175, 2000, 2023, 2300...  \n",
       "..                                                 ...  \n",
       "255  [101, 1000, 2947, 1045, 2938, 1010, 2004, 2014...  \n",
       "256  [101, 2388, 1010, 1045, 3198, 2017, 2320, 2062...  \n",
       "257  [101, 1000, 1005, 2021, 1045, 2113, 2870, 1010...  \n",
       "258  [101, 1000, 2054, 2003, 1996, 2204, 1997, 1789...  \n",
       "259  [101, 2129, 2071, 1045, 3637, 1999, 1996, 2793...  \n",
       "\n",
       "[260 rows x 6 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read in data\n",
    "df = pd.read_csv('cleaned_data.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66cda6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dict to key label strings to numbers\n",
    "labels = [\"noble_agented\", \"guilt_penance_self_loathing\", \"senseless_meaning_making\", \"romantic_reunion\", \"hopeless_empty_agony\", \"cowardice_foolish_unpleasant\"]\n",
    "vals = range(0, len(labels))\n",
    "labels_dict = {k:int(v) for k,v in zip(labels,vals)}\n",
    "#initialize empty col for labels as numbers\n",
    "df[\"lab_nums\"] = np.zeros(len(df[\"ind\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad212b74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-dfb1a9247153>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"lab_nums\"][i] = int(labels_dict[label])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>ind</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>tok_nums</th>\n",
       "      <th>lab_nums</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>thegoodsoldier</td>\n",
       "      <td>leonora, however, was not in the least keen on...</td>\n",
       "      <td>noble_agented</td>\n",
       "      <td>[101, 6506, 6525, 1010, 2174, 1010, 2001, 2025...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>thegoodsoldier</td>\n",
       "      <td>\"i said a great deal more to him than i wante...</td>\n",
       "      <td>guilt_penance_self_loathing</td>\n",
       "      <td>[101, 1000, 1045, 2056, 1037, 2307, 3066, 2062...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>hamlet</td>\n",
       "      <td>one woe doth tread upon another's heel, so fas...</td>\n",
       "      <td>senseless_meaning_making</td>\n",
       "      <td>[101, 2028, 24185, 2063, 11089, 2232, 29449, 2...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>hamlet</td>\n",
       "      <td>how can that be, unless she drowned herself in...</td>\n",
       "      <td>noble_agented</td>\n",
       "      <td>[101, 2129, 2064, 2008, 2022, 1010, 4983, 2016...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>hamlet</td>\n",
       "      <td>if the man go to this water and drown himself,...</td>\n",
       "      <td>noble_agented</td>\n",
       "      <td>[101, 2065, 1996, 2158, 2175, 2000, 2023, 2300...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>255</td>\n",
       "      <td>255</td>\n",
       "      <td>thewish</td>\n",
       "      <td>\\n\"Thus I sat, as her murderess, at her bedsid...</td>\n",
       "      <td>guilt_penance_self_loathing</td>\n",
       "      <td>[101, 1000, 2947, 1045, 2938, 1010, 2004, 2014...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>256</td>\n",
       "      <td>256</td>\n",
       "      <td>thewish</td>\n",
       "      <td>Mother, I ask you once more: why did she die?\"...</td>\n",
       "      <td>cowardice_foolish_unpleasant</td>\n",
       "      <td>[101, 2388, 1010, 1045, 3198, 2017, 2320, 2062...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257</th>\n",
       "      <td>257</td>\n",
       "      <td>257</td>\n",
       "      <td>thewish</td>\n",
       "      <td>\"'But I know myself,' said she. 'I break it of...</td>\n",
       "      <td>guilt_penance_self_loathing</td>\n",
       "      <td>[101, 1000, 1005, 2021, 1045, 2113, 2870, 1010...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>258</td>\n",
       "      <td>258</td>\n",
       "      <td>thewish</td>\n",
       "      <td>\"What is the good of clinging to happiness whe...</td>\n",
       "      <td>guilt_penance_self_loathing</td>\n",
       "      <td>[101, 1000, 2054, 2003, 1996, 2204, 1997, 1789...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>259</td>\n",
       "      <td>259</td>\n",
       "      <td>thewish</td>\n",
       "      <td>How could I sleep in the bed out of which I wi...</td>\n",
       "      <td>guilt_penance_self_loathing</td>\n",
       "      <td>[101, 2129, 2071, 1045, 3637, 1999, 1996, 2793...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>260 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0  ind           title  \\\n",
       "0             0    0  thegoodsoldier   \n",
       "1             1    1  thegoodsoldier   \n",
       "2             2    2          hamlet   \n",
       "3             3    3          hamlet   \n",
       "4             4    4          hamlet   \n",
       "..          ...  ...             ...   \n",
       "255         255  255         thewish   \n",
       "256         256  256         thewish   \n",
       "257         257  257         thewish   \n",
       "258         258  258         thewish   \n",
       "259         259  259         thewish   \n",
       "\n",
       "                                                  text  \\\n",
       "0    leonora, however, was not in the least keen on...   \n",
       "1     \"i said a great deal more to him than i wante...   \n",
       "2    one woe doth tread upon another's heel, so fas...   \n",
       "3    how can that be, unless she drowned herself in...   \n",
       "4    if the man go to this water and drown himself,...   \n",
       "..                                                 ...   \n",
       "255  \\n\"Thus I sat, as her murderess, at her bedsid...   \n",
       "256  Mother, I ask you once more: why did she die?\"...   \n",
       "257  \"'But I know myself,' said she. 'I break it of...   \n",
       "258  \"What is the good of clinging to happiness whe...   \n",
       "259  How could I sleep in the bed out of which I wi...   \n",
       "\n",
       "                            label  \\\n",
       "0                   noble_agented   \n",
       "1     guilt_penance_self_loathing   \n",
       "2        senseless_meaning_making   \n",
       "3                   noble_agented   \n",
       "4                   noble_agented   \n",
       "..                            ...   \n",
       "255   guilt_penance_self_loathing   \n",
       "256  cowardice_foolish_unpleasant   \n",
       "257   guilt_penance_self_loathing   \n",
       "258   guilt_penance_self_loathing   \n",
       "259   guilt_penance_self_loathing   \n",
       "\n",
       "                                              tok_nums  lab_nums  \n",
       "0    [101, 6506, 6525, 1010, 2174, 1010, 2001, 2025...       0.0  \n",
       "1    [101, 1000, 1045, 2056, 1037, 2307, 3066, 2062...       1.0  \n",
       "2    [101, 2028, 24185, 2063, 11089, 2232, 29449, 2...       2.0  \n",
       "3    [101, 2129, 2064, 2008, 2022, 1010, 4983, 2016...       0.0  \n",
       "4    [101, 2065, 1996, 2158, 2175, 2000, 2023, 2300...       0.0  \n",
       "..                                                 ...       ...  \n",
       "255  [101, 1000, 2947, 1045, 2938, 1010, 2004, 2014...       1.0  \n",
       "256  [101, 2388, 1010, 1045, 3198, 2017, 2320, 2062...       5.0  \n",
       "257  [101, 1000, 1005, 2021, 1045, 2113, 2870, 1010...       1.0  \n",
       "258  [101, 1000, 2054, 2003, 1996, 2204, 1997, 1789...       1.0  \n",
       "259  [101, 2129, 2071, 1045, 3637, 1999, 1996, 2793...       1.0  \n",
       "\n",
       "[260 rows x 7 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fill empty lab_nums col\n",
    "for i,label in enumerate(df[\"label\"]):\n",
    "    df[\"lab_nums\"][i] = int(labels_dict[label])\n",
    "    \n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d4a1270",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>leonora, however, was not in the least keen on...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"i said a great deal more to him than i wante...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>one woe doth tread upon another's heel, so fas...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>how can that be, unless she drowned herself in...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>if the man go to this water and drown himself,...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>\\n\"Thus I sat, as her murderess, at her bedsid...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>Mother, I ask you once more: why did she die?\"...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257</th>\n",
       "      <td>\"'But I know myself,' said she. 'I break it of...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>\"What is the good of clinging to happiness whe...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>How could I sleep in the bed out of which I wi...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>260 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              sentence  label\n",
       "0    leonora, however, was not in the least keen on...    0.0\n",
       "1     \"i said a great deal more to him than i wante...    1.0\n",
       "2    one woe doth tread upon another's heel, so fas...    2.0\n",
       "3    how can that be, unless she drowned herself in...    0.0\n",
       "4    if the man go to this water and drown himself,...    0.0\n",
       "..                                                 ...    ...\n",
       "255  \\n\"Thus I sat, as her murderess, at her bedsid...    1.0\n",
       "256  Mother, I ask you once more: why did she die?\"...    5.0\n",
       "257  \"'But I know myself,' said she. 'I break it of...    1.0\n",
       "258  \"What is the good of clinging to happiness whe...    1.0\n",
       "259  How could I sleep in the bed out of which I wi...    1.0\n",
       "\n",
       "[260 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#subset data to what's necessary for training\n",
    "data_df = pd.DataFrame(columns=[\"sentence\", \"label\"])    \n",
    "data_df[\"sentence\"] = df[\"text\"]\n",
    "data_df[\"label\"] = df[\"lab_nums\"]\n",
    "\n",
    "#save to csv as checkpoint\n",
    "data_df.to_csv('kfold_dataset.csv')     \n",
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89fd3c6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-c09c15a55d837c45\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to /Users/umadwivedi/.cache/huggingface/datasets/csv/default-c09c15a55d837c45/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75478bebd08e4c24a0fc1f2405a37163",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "144bb66011df4a47b534ab8acf35b56a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to /Users/umadwivedi/.cache/huggingface/datasets/csv/default-c09c15a55d837c45/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c458c0d7a7494268b3f997691ef8020b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-c09c15a55d837c45\n",
      "Reusing dataset csv (/Users/umadwivedi/.cache/huggingface/datasets/csv/default-c09c15a55d837c45/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bceee7c5f2ec4a278b1c7757a7fe312b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#create 10 train/dev splits\n",
    "vals_ds = datasets.load_dataset('csv', data_files='kfold_dataset.csv',split=[\n",
    "    f'train[{k}%:{k+10}%]' for k in range(0, 100, 10)\n",
    "])\n",
    "trains_ds = datasets.load_dataset('csv', data_files='kfold_dataset.csv',split=[\n",
    "    f'train[:{k}%]+train[{k+10}%:]' for k in range(0, 100, 10)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40a0afff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set hyper-hyperparameters\n",
    "model_name = \"bert-base-uncased\"\n",
    "max_length = 3000\n",
    "#load tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_name, do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2f596de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#format tokenized training texts in dict\n",
    "train_texts = {}\n",
    "for i in range(len(trains_ds)):\n",
    "    texts=[]\n",
    "    for j in range(len(trains_ds[i])):\n",
    "        d = tokenizer(trains_ds[i][j]['sentence'])\n",
    "        label = np.zeros(6)\n",
    "        label[int(trains_ds[i][j]['label'])] = 1\n",
    "        scal = int(trains_ds[i][j]['label'])\n",
    "        d['label'] = scal\n",
    "        texts.append(d)\n",
    "    train_texts[str(i)] = texts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d286c012",
   "metadata": {},
   "outputs": [],
   "source": [
    "#format tokenized validation texts in dict\n",
    "val_texts = {}\n",
    "for i in range(len(vals_ds)):\n",
    "    texts=[]\n",
    "    for j in range(len(vals_ds[i])):\n",
    "        d = tokenizer(vals_ds[i][j]['sentence'])\n",
    "        label = np.zeros(6)\n",
    "        label[int(vals_ds[i][j]['label'])] = 1\n",
    "        scal = int(vals_ds[i][j]['label'])\n",
    "        d['label'] = scal\n",
    "        texts.append(d)\n",
    "    val_texts[str(i)] = texts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "abcfac93",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compile training and validation in one dict\n",
    "dat = {\"train\": train_texts, \"validation\": val_texts}\n",
    "#make datasetdict object\n",
    "dataset = datasets.DatasetDict(dat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "91088250",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#instantiate model\n",
    "num_labels = 6\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels, output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "307fafa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set model training arguments\n",
    "model_name = model_name.split(\"/\")[-1]\n",
    "\n",
    "args = TrainingArguments(\n",
    "    f\"{model_name}-finetuned-{'suicide'}\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=9,\n",
    "    per_device_eval_batch_size=2,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    logging_dir='./logs',\n",
    "    push_to_hub=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "60f6bfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define function to compute metrics\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = np.argmax(pred.predictions[0], axis=1)\n",
    "    metric = datasets.load_metric('accuracy')\n",
    "    metric.add_batch(predictions=preds, references=labels)\n",
    "    acc = metric.compute()\n",
    "    return acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b18cb9d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/umadwivedi/Documents/Projects/YALE/senior/s1/LING 380 â€” neural network language models/final_project/bert-base-uncased-finetuned-suicide is already a clone of https://huggingface.co/psychicautomaton/bert-base-uncased-finetuned-suicide. Make sure you pull the latest changes with `repo.git_pull()`.\n",
      "***** Running training *****\n",
      "  Num examples = 234\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 9\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 9\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 260\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpsychicautomaton\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/psychicautomaton/huggingface/runs/1f82xuhd\" target=\"_blank\">bert-base-uncased-finetuned-suicide</a></strong> to <a href=\"https://wandb.ai/psychicautomaton/huggingface\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='260' max='260' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [260/260 43:48, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.783373</td>\n",
       "      <td>0.115385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.569633</td>\n",
       "      <td>0.307692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.545996</td>\n",
       "      <td>0.230769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.658986</td>\n",
       "      <td>0.230769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.428601</td>\n",
       "      <td>0.384615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.442988</td>\n",
       "      <td>0.461538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.417432</td>\n",
       "      <td>0.384615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.287315</td>\n",
       "      <td>0.576923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.312404</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.334946</td>\n",
       "      <td>0.538462</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 26\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to bert-base-uncased-finetuned-suicide/checkpoint-26\n",
      "Configuration saved in bert-base-uncased-finetuned-suicide/checkpoint-26/config.json\n",
      "Model weights saved in bert-base-uncased-finetuned-suicide/checkpoint-26/pytorch_model.bin\n",
      "tokenizer config file saved in bert-base-uncased-finetuned-suicide/checkpoint-26/tokenizer_config.json\n",
      "Special tokens file saved in bert-base-uncased-finetuned-suicide/checkpoint-26/special_tokens_map.json\n",
      "tokenizer config file saved in bert-base-uncased-finetuned-suicide/tokenizer_config.json\n",
      "Special tokens file saved in bert-base-uncased-finetuned-suicide/special_tokens_map.json\n",
      "Several commits (3) will be pushed upstream.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 26\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to bert-base-uncased-finetuned-suicide/checkpoint-52\n",
      "Configuration saved in bert-base-uncased-finetuned-suicide/checkpoint-52/config.json\n",
      "Model weights saved in bert-base-uncased-finetuned-suicide/checkpoint-52/pytorch_model.bin\n",
      "tokenizer config file saved in bert-base-uncased-finetuned-suicide/checkpoint-52/tokenizer_config.json\n",
      "Special tokens file saved in bert-base-uncased-finetuned-suicide/checkpoint-52/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 26\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to bert-base-uncased-finetuned-suicide/checkpoint-78\n",
      "Configuration saved in bert-base-uncased-finetuned-suicide/checkpoint-78/config.json\n",
      "Model weights saved in bert-base-uncased-finetuned-suicide/checkpoint-78/pytorch_model.bin\n",
      "tokenizer config file saved in bert-base-uncased-finetuned-suicide/checkpoint-78/tokenizer_config.json\n",
      "Special tokens file saved in bert-base-uncased-finetuned-suicide/checkpoint-78/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 26\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to bert-base-uncased-finetuned-suicide/checkpoint-104\n",
      "Configuration saved in bert-base-uncased-finetuned-suicide/checkpoint-104/config.json\n",
      "Model weights saved in bert-base-uncased-finetuned-suicide/checkpoint-104/pytorch_model.bin\n",
      "tokenizer config file saved in bert-base-uncased-finetuned-suicide/checkpoint-104/tokenizer_config.json\n",
      "Special tokens file saved in bert-base-uncased-finetuned-suicide/checkpoint-104/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 26\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to bert-base-uncased-finetuned-suicide/checkpoint-130\n",
      "Configuration saved in bert-base-uncased-finetuned-suicide/checkpoint-130/config.json\n",
      "Model weights saved in bert-base-uncased-finetuned-suicide/checkpoint-130/pytorch_model.bin\n",
      "tokenizer config file saved in bert-base-uncased-finetuned-suicide/checkpoint-130/tokenizer_config.json\n",
      "Special tokens file saved in bert-base-uncased-finetuned-suicide/checkpoint-130/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 26\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to bert-base-uncased-finetuned-suicide/checkpoint-156\n",
      "Configuration saved in bert-base-uncased-finetuned-suicide/checkpoint-156/config.json\n",
      "Model weights saved in bert-base-uncased-finetuned-suicide/checkpoint-156/pytorch_model.bin\n",
      "tokenizer config file saved in bert-base-uncased-finetuned-suicide/checkpoint-156/tokenizer_config.json\n",
      "Special tokens file saved in bert-base-uncased-finetuned-suicide/checkpoint-156/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 26\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to bert-base-uncased-finetuned-suicide/checkpoint-182\n",
      "Configuration saved in bert-base-uncased-finetuned-suicide/checkpoint-182/config.json\n",
      "Model weights saved in bert-base-uncased-finetuned-suicide/checkpoint-182/pytorch_model.bin\n",
      "tokenizer config file saved in bert-base-uncased-finetuned-suicide/checkpoint-182/tokenizer_config.json\n",
      "Special tokens file saved in bert-base-uncased-finetuned-suicide/checkpoint-182/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 26\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to bert-base-uncased-finetuned-suicide/checkpoint-208\n",
      "Configuration saved in bert-base-uncased-finetuned-suicide/checkpoint-208/config.json\n",
      "Model weights saved in bert-base-uncased-finetuned-suicide/checkpoint-208/pytorch_model.bin\n",
      "tokenizer config file saved in bert-base-uncased-finetuned-suicide/checkpoint-208/tokenizer_config.json\n",
      "Special tokens file saved in bert-base-uncased-finetuned-suicide/checkpoint-208/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 26\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to bert-base-uncased-finetuned-suicide/checkpoint-234\n",
      "Configuration saved in bert-base-uncased-finetuned-suicide/checkpoint-234/config.json\n",
      "Model weights saved in bert-base-uncased-finetuned-suicide/checkpoint-234/pytorch_model.bin\n",
      "tokenizer config file saved in bert-base-uncased-finetuned-suicide/checkpoint-234/tokenizer_config.json\n",
      "Special tokens file saved in bert-base-uncased-finetuned-suicide/checkpoint-234/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 26\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to bert-base-uncased-finetuned-suicide/checkpoint-260\n",
      "Configuration saved in bert-base-uncased-finetuned-suicide/checkpoint-260/config.json\n",
      "Model weights saved in bert-base-uncased-finetuned-suicide/checkpoint-260/pytorch_model.bin\n",
      "tokenizer config file saved in bert-base-uncased-finetuned-suicide/checkpoint-260/tokenizer_config.json\n",
      "Special tokens file saved in bert-base-uncased-finetuned-suicide/checkpoint-260/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from bert-base-uncased-finetuned-suicide/checkpoint-208 (score: 0.5769230769230769).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=260, training_loss=1.1855906559870792, metrics={'train_runtime': 2646.1149, 'train_samples_per_second': 0.884, 'train_steps_per_second': 0.098, 'total_flos': 196944156145044.0, 'train_loss': 1.1855906559870792, 'epoch': 10.0})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#select data split randomly\n",
    "ind = random.sample(range(len(dataset['train'].keys())), 1)[0]\n",
    "\n",
    "#instatiate model trainer\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    args=args,\n",
    "    train_dataset=dataset[\"train\"][str(ind)],\n",
    "    eval_dataset=dataset[\"validation\"][str(ind)],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics = compute_metrics\n",
    ")\n",
    "#train model    \n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d6ce9abd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#look at model structure\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a3ada21b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /Users/umadwivedi/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.12.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /Users/umadwivedi/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "#load trained model from checkpoint so you don't have to repeat training\n",
    "#-- note! instructors will not be able to do this\n",
    "#from the github repo because the bin model file would not upload bc it's too big\n",
    "#you should be fine with the zipped directory on canvas\n",
    "output_model_file = 'bert-base-uncased-finetuned-suicide/pytorch_model.bin'\n",
    "model_state_dict = torch.load(output_model_file) \n",
    "model = AutoModelForSequenceClassification.from_pretrained('bert-base-uncased', state_dict=model_state_dict, num_labels=6,output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b27b2b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get word to BERT token index dict\n",
    "with open('tokenizer.json') as json_file:\n",
    "    vocab = json.load(json_file)['model']['vocab']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "af4d0701",
   "metadata": {},
   "outputs": [],
   "source": [
    "#words of interest\n",
    "suicide_list = [\"suicide\", \n",
    "                \"courage\", \n",
    "                \"guilt\", \n",
    "                \"shame\", \n",
    "                \"joy\", \n",
    "                \"ecstasy\", \n",
    "                \"despair\", \n",
    "                \"misery\", \n",
    "                \"romance\", \n",
    "                \"agency\", \n",
    "                \"nobility\",\n",
    "                \"meaning\",\n",
    "                \"meaningless\",\n",
    "                \"meaningful\",\n",
    "                \"punishment\",\n",
    "                \"hopeless\",\n",
    "                \"hope\",\n",
    "                \"escape\"]\n",
    "\n",
    "#fill dict with word of interest and its token index in embeddings\n",
    "suicide_dict = {}\n",
    "\n",
    "for word in suicide_list:\n",
    "    suicide_dict[word] = vocab[word]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3e552d7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(30522, 768, padding_idx=0)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get pretrained embeddings\n",
    "gen_embeds = model.get_input_embeddings()\n",
    "gen_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "561f4622",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get learned weights\n",
    "em_weights = model_state_dict['bert.embeddings.word_embeddings.weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1537ac08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get new embeds\n",
    "embeds_dict = {}\n",
    "\n",
    "for i in range(len(suicide_dict.keys())):\n",
    "    word = list(suicide_dict.keys())[i]\n",
    "    ind = suicide_dict[word]\n",
    "    lookup_tensor = torch.tensor([suicide_dict[word]], dtype=torch.long)\n",
    "    raw = gen_embeds(lookup_tensor).detach().numpy()\n",
    "    embeds_dict[word] = raw*np.array(em_weights[ind])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2d2efece",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dict of pretrained embeddings\n",
    "gen_embeds_dict = {}\n",
    "\n",
    "for i in range(len(suicide_dict.keys())):\n",
    "    word = list(suicide_dict.keys())[i]\n",
    "    lookup_tensor = torch.tensor([suicide_dict[word]], dtype=torch.long)\n",
    "    gen_embeds_dict[word] = gen_embeds(lookup_tensor).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "487c0905",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3.3231e-02, -1.5140e-02,  7.0230e-02,  1.1353e-02,  9.2188e-02,\n",
       "          5.0092e-02, -7.4500e-03,  6.2658e-02, -3.3872e-02,  2.0832e-02,\n",
       "          3.9862e-02,  8.1161e-02,  3.1511e-02,  1.0292e-02, -1.3431e-02,\n",
       "          3.8536e-02,  2.4391e-02,  1.9971e-02, -3.4887e-02, -2.8261e-02,\n",
       "          2.8978e-02,  8.6722e-02, -2.3939e-03,  4.9538e-02,  4.7348e-03,\n",
       "          1.0622e-01,  3.6979e-02, -1.9733e-03,  2.2047e-02,  6.1249e-02,\n",
       "          1.1682e-02, -1.2333e-02,  3.8076e-03,  2.5259e-02, -2.9316e-02,\n",
       "          1.5769e-02,  5.0311e-02, -2.1167e-02, -2.2294e-03,  1.3587e-02,\n",
       "          7.6778e-02,  6.3121e-02, -9.6975e-03,  4.2445e-02, -8.1854e-03,\n",
       "          4.5742e-02,  4.2900e-03,  4.2281e-02,  2.1551e-03,  2.0152e-02,\n",
       "         -1.5598e-02, -4.9590e-02,  8.0576e-02, -3.9811e-02, -2.8291e-02,\n",
       "          1.2898e-01, -2.6500e-02,  2.9023e-02,  2.8383e-02,  9.3874e-02,\n",
       "          4.4358e-03,  3.8665e-02, -6.3605e-03,  2.8731e-02,  1.3130e-02,\n",
       "          4.3289e-02, -1.5431e-04,  3.4130e-02,  1.9133e-02,  1.2356e-02,\n",
       "          2.6053e-02,  8.1477e-02,  1.0457e-02,  3.8015e-02,  5.7919e-02,\n",
       "         -6.1980e-03,  1.5863e-01, -4.7156e-03, -8.5924e-03,  8.6134e-02,\n",
       "          4.6684e-02,  6.4950e-02,  8.0211e-02,  1.8761e-02,  1.6152e-02,\n",
       "         -2.6611e-02, -3.6679e-02,  9.1987e-02, -4.7318e-02,  2.2043e-02,\n",
       "          5.8040e-02,  3.4125e-02,  1.5990e-02,  2.6392e-02,  5.7530e-02,\n",
       "          2.8298e-02,  2.0027e-02,  2.8120e-03,  4.6672e-02,  2.3773e-02,\n",
       "         -9.4943e-03, -1.7021e-02,  1.1794e-01,  4.8436e-02,  4.9295e-02,\n",
       "         -2.5453e-02,  2.2159e-02,  5.8520e-02, -4.1227e-02, -4.8368e-02,\n",
       "          5.8477e-02,  1.9699e-02,  3.4856e-02,  7.6723e-02, -6.4375e-02,\n",
       "          1.5149e-02,  2.6840e-02,  3.6635e-02,  2.7181e-02,  6.9519e-02,\n",
       "          4.1182e-02, -6.0348e-02,  4.2464e-02,  4.1034e-02,  1.1501e-01,\n",
       "          5.1798e-02,  7.7080e-03,  1.4669e-01, -2.1348e-02,  9.1454e-02,\n",
       "          1.4657e-02,  5.0325e-02, -6.6835e-03,  5.7321e-02, -3.7560e-02,\n",
       "          1.1508e-02,  1.5741e-02,  3.0558e-02,  1.6204e-02, -1.7671e-02,\n",
       "          5.6191e-02, -4.1505e-02, -4.7328e-03,  1.9843e-02,  2.1192e-02,\n",
       "          2.3808e-02,  1.2203e-02,  2.7536e-02,  2.8110e-02,  3.1378e-02,\n",
       "          8.1274e-04,  6.8844e-02,  4.9304e-02,  3.9350e-02,  5.0292e-02,\n",
       "         -4.7421e-03, -2.0259e-02,  8.3564e-03,  2.3708e-02, -2.9683e-03,\n",
       "          3.3816e-02,  1.2213e-02,  2.9591e-02,  1.0594e-02,  1.6532e-02,\n",
       "         -5.3419e-03,  1.0084e-01,  5.3054e-02, -2.2449e-02, -1.8963e-02,\n",
       "          4.9256e-02,  8.4973e-02,  1.9072e-02,  5.9358e-02, -2.4521e-02,\n",
       "         -2.9881e-02,  5.8206e-02,  1.0505e-01,  3.0957e-02,  3.5956e-02,\n",
       "         -2.2543e-02, -2.1739e-02,  6.5549e-02, -6.4026e-02,  7.4230e-02,\n",
       "          6.8036e-02,  1.3139e-01, -2.2496e-02,  1.7778e-02, -4.0667e-02,\n",
       "         -2.7318e-03, -3.7514e-03,  3.2566e-02,  4.0001e-02,  5.5598e-02,\n",
       "          2.9119e-02,  6.7389e-02, -1.2043e-02, -7.3118e-04,  3.5040e-02,\n",
       "          2.2019e-02, -1.4373e-02,  4.9892e-03,  6.6877e-02,  4.4560e-02,\n",
       "          3.3856e-03, -2.8849e-02, -2.6195e-02,  7.1447e-02,  3.2878e-02,\n",
       "          6.0577e-02,  1.0300e-01,  1.5409e-02, -2.4263e-02,  1.9569e-04,\n",
       "         -4.3856e-03,  5.3112e-02,  7.1288e-02,  1.4783e-02,  1.6949e-02,\n",
       "         -3.9204e-02, -4.8395e-02,  2.6596e-02, -4.1145e-02,  2.5802e-02,\n",
       "         -3.2851e-02,  4.1548e-02,  3.4513e-02,  3.7086e-02,  4.1342e-02,\n",
       "          2.5490e-02,  4.7357e-02,  6.5986e-02, -1.3849e-02,  1.7472e-02,\n",
       "         -3.1982e-02,  2.8500e-02, -1.9026e-03,  5.1383e-02,  5.2046e-02,\n",
       "          5.8714e-02,  6.8333e-02,  2.5800e-02,  6.6805e-03,  7.1534e-02,\n",
       "         -4.0952e-02,  4.7618e-02,  5.1535e-02,  7.0710e-02,  9.7544e-02,\n",
       "          3.3577e-02, -1.5237e-02, -8.6773e-04, -1.3890e-02,  5.9102e-02,\n",
       "          1.8708e-02, -1.1436e-02, -2.3472e-02,  1.7103e-02,  2.4700e-02,\n",
       "          1.0167e-01,  5.4955e-02, -3.8345e-02, -5.5743e-02,  4.0989e-02,\n",
       "          3.3492e-02,  6.3351e-02, -1.6772e-02, -1.5878e-02, -2.9157e-02,\n",
       "          5.4279e-02,  3.1746e-02,  8.7912e-02,  3.5773e-02,  9.0691e-03,\n",
       "          1.0459e-04, -8.2048e-03,  6.6452e-02,  3.7162e-02, -2.7738e-02,\n",
       "          7.5279e-02,  3.0666e-02,  4.4616e-02, -2.1513e-02,  5.2338e-02,\n",
       "          4.3912e-02,  1.0791e-02,  2.7190e-02, -6.7654e-03,  9.0870e-02,\n",
       "          5.5613e-02, -2.9465e-02,  1.1661e-03, -2.5784e-02,  6.1574e-02,\n",
       "          6.4083e-03,  2.5454e-03,  4.6046e-02,  8.8028e-02, -1.3291e-02,\n",
       "         -6.1489e-03,  1.0925e-02, -4.7765e-02,  2.2703e-02, -1.4817e-02,\n",
       "          1.2919e-02, -4.5643e-02, -6.0824e-02, -4.5261e-02, -6.2666e-02,\n",
       "          1.0275e-01, -5.9616e-02,  4.9705e-02,  3.9907e-02,  1.0013e-01,\n",
       "          1.6190e-02,  5.2660e-02,  6.8875e-02,  5.1875e-03, -5.4424e-02,\n",
       "          1.0963e-02,  2.1864e-02,  1.1346e-01, -2.4011e-03,  5.8639e-02,\n",
       "          6.8120e-04,  1.9850e-02,  7.3669e-02,  4.7158e-02, -1.1245e-02,\n",
       "          9.1798e-03, -3.2122e-02,  2.4249e-02, -3.4813e-04,  2.3739e-02,\n",
       "          7.9727e-02,  5.9324e-03,  5.7803e-02,  4.2412e-02,  3.0931e-02,\n",
       "          3.3047e-02, -4.1434e-02,  8.4203e-02,  1.1234e-02, -1.5614e-02,\n",
       "          8.0731e-03,  3.1695e-02,  5.9153e-03,  2.4428e-02, -3.8916e-02,\n",
       "          3.0786e-03, -3.1725e-02,  1.0590e-01, -4.2226e-02,  4.0003e-02,\n",
       "          3.4670e-02,  2.5118e-02,  3.0504e-02,  2.5185e-02,  2.5022e-02,\n",
       "         -6.1209e-03,  3.5580e-02, -5.3710e-02,  1.1956e-02,  6.7446e-02,\n",
       "         -5.4610e-02,  3.4326e-02,  8.2337e-02,  2.1359e-02,  5.1694e-02,\n",
       "         -1.7663e-03,  3.6069e-02,  6.7066e-02,  5.9986e-02,  3.3600e-02,\n",
       "         -7.3331e-03,  5.3394e-02,  4.7850e-02,  1.1935e-01,  2.7370e-02,\n",
       "          6.1891e-02,  3.3257e-02,  7.1323e-02,  1.8790e-02,  4.0946e-02,\n",
       "          9.8274e-02,  3.1052e-02, -2.9398e-02,  9.2097e-02, -3.6932e-02,\n",
       "          4.5866e-02,  3.5989e-02,  2.9056e-02,  4.6057e-02,  7.9247e-02,\n",
       "         -3.7947e-02,  2.7719e-02,  9.0238e-02,  1.6214e-02, -7.3115e-03,\n",
       "          3.1512e-02,  1.3289e-01,  3.8510e-02,  3.6813e-02,  4.3620e-02,\n",
       "          4.3282e-02,  5.1069e-02, -1.0903e-02, -4.6947e-02,  3.2024e-02,\n",
       "          9.0239e-02,  5.6669e-02, -1.6719e-02,  5.0550e-04, -1.1497e-02,\n",
       "         -3.2219e-02,  5.6277e-02,  6.5364e-02,  6.3739e-02,  2.7350e-02,\n",
       "         -3.3348e-02,  2.9016e-02, -1.8020e-02,  4.4947e-02,  3.3437e-02,\n",
       "          8.8266e-02,  6.6195e-02,  8.1027e-02,  6.5126e-02,  5.0082e-02,\n",
       "          4.3082e-02,  8.7698e-02, -3.5180e-03,  3.8748e-02, -1.4277e-02,\n",
       "         -3.4023e-02,  2.0274e-02,  7.9505e-02,  1.1765e-02, -1.6989e-02,\n",
       "          1.9920e-03,  3.4363e-02, -3.5143e-03, -1.9839e-02, -1.6189e-04,\n",
       "          9.0797e-02,  1.1687e-02, -1.9831e-03, -2.6142e-02,  7.1379e-02,\n",
       "         -1.4864e-02, -3.3167e-02, -3.6905e-03,  3.9447e-02,  5.2765e-02,\n",
       "          3.2519e-02,  3.6685e-02,  1.2208e-03,  6.5273e-02,  1.0237e-01,\n",
       "          4.4465e-02,  4.4690e-03,  4.6480e-02, -5.1899e-02, -5.4198e-03,\n",
       "         -2.5038e-02,  6.3162e-02, -2.1092e-03, -4.4139e-02,  2.9795e-02,\n",
       "          1.0278e-02,  6.5079e-03,  4.1077e-02, -4.9345e-02,  2.0948e-02,\n",
       "         -2.7495e-03,  4.6773e-02,  6.7042e-02, -4.0789e-02, -1.5700e-02,\n",
       "          1.3004e-01,  7.1905e-02, -5.0589e-02,  2.8474e-02, -3.5853e-03,\n",
       "          2.2540e-03,  2.2851e-02,  9.9278e-02, -1.4649e-03,  6.3218e-02,\n",
       "          3.4383e-02,  7.9789e-02,  4.0624e-02,  1.2429e-01, -9.0256e-03,\n",
       "         -4.8464e-03, -1.0691e-02,  7.8622e-03,  4.8293e-02,  7.2330e-02,\n",
       "         -1.2643e-02,  6.8011e-02,  1.2564e-03,  9.8349e-02,  1.0805e-01,\n",
       "          4.1628e-02,  1.2522e-02, -2.8957e-02, -1.6758e-02, -2.2165e-02,\n",
       "          1.0041e-01,  5.5325e-02,  9.5056e-02, -1.6046e-02,  8.2428e-02,\n",
       "          8.0148e-02, -3.6618e-03, -3.7657e-02,  6.3069e-02,  1.7879e-02,\n",
       "          7.0509e-02,  3.2534e-02, -3.7305e-02, -5.3956e-02,  1.2050e-01,\n",
       "          1.2537e-01, -1.9263e-02,  4.0726e-02,  2.6634e-02, -4.5125e-02,\n",
       "          1.3119e-02,  4.0830e-02,  4.1087e-02,  7.1793e-02,  4.4488e-02,\n",
       "         -3.6206e-02, -1.5250e-02,  3.8053e-02,  3.5704e-02,  1.0098e-02,\n",
       "          1.7868e-02, -5.8344e-02,  9.9110e-02,  9.9390e-02,  1.7019e-02,\n",
       "          1.3325e-02, -5.9564e-02, -2.6044e-02,  4.9925e-02,  1.9992e-02,\n",
       "          2.5078e-02,  4.7047e-02,  3.8698e-02,  5.6882e-02, -1.2946e-02,\n",
       "         -1.6553e-02,  1.1811e-01,  6.5877e-02,  8.9627e-02,  7.6246e-02,\n",
       "         -1.4469e-03,  4.5380e-02,  4.2220e-02,  4.1855e-02, -4.0978e-02,\n",
       "          3.3241e-02, -8.9225e-04,  3.5285e-02,  4.0454e-02, -2.8939e-03,\n",
       "         -4.4739e-02,  4.7226e-02, -1.8424e-02, -2.3018e-02, -2.5404e-02,\n",
       "          8.0189e-02, -6.4093e-03, -3.8724e-02, -5.3237e-02,  1.0847e-01,\n",
       "         -4.3598e-02, -3.7079e-02,  5.2572e-02,  1.7639e-02, -4.1866e-03,\n",
       "          4.3154e-02,  1.9890e-02,  1.3937e-02,  1.0876e-01, -3.1490e-02,\n",
       "          3.6290e-02,  2.9470e-02,  2.0145e-02, -2.8240e-02,  1.6451e-02,\n",
       "         -3.2508e-03, -4.6595e-02,  1.1739e-01, -4.1246e-02,  7.1701e-02,\n",
       "          6.6814e-02,  8.6883e-03,  1.6086e-02, -2.4517e-02,  1.4427e-01,\n",
       "         -1.1901e-02,  4.5930e-02,  1.1879e-01, -3.1642e-02,  3.2048e-02,\n",
       "          2.8536e-02,  5.9558e-02, -6.8412e-02,  4.4069e-03, -1.6128e-02,\n",
       "          7.9227e-02,  1.8491e-02,  4.1094e-02,  1.0528e-01,  1.0732e-02,\n",
       "          1.0770e-02, -4.3913e-02,  5.4749e-02, -3.0307e-02, -4.7667e-03,\n",
       "          8.0757e-02, -2.3848e-02,  6.0505e-02, -1.4078e-02,  1.4633e-02,\n",
       "         -1.4849e-02, -1.1847e-02,  1.1760e-01,  1.0635e-01, -4.4285e-02,\n",
       "          1.3201e-01,  1.9146e-02, -3.2753e-02,  1.2955e-02,  9.4111e-02,\n",
       "          3.9748e-02,  1.0831e-01,  2.9358e-03, -7.8584e-03, -5.3712e-03,\n",
       "         -5.5315e-02, -2.8182e-02,  6.7306e-02,  7.4868e-02, -3.0397e-02,\n",
       "          3.0808e-03,  3.6944e-02,  4.8179e-03,  1.2229e-01, -3.6480e-02,\n",
       "         -7.3216e-03, -1.9143e-02, -2.9520e-03,  4.4213e-02, -1.0348e-02,\n",
       "          2.4429e-02,  6.2211e-02,  5.9596e-02,  1.8193e-02,  3.2476e-02,\n",
       "          2.0520e-02,  6.1469e-02,  1.2257e-02, -1.8139e-02,  1.1807e-01,\n",
       "         -1.8451e-02,  5.7571e-02,  3.7449e-02,  3.9348e-02,  1.1989e-02,\n",
       "          3.4268e-02,  2.7567e-02, -2.3342e-02, -7.5678e-04, -1.1153e-02,\n",
       "          2.8898e-02,  3.7868e-03,  5.2969e-03,  2.3645e-02,  1.5864e-02,\n",
       "         -1.8599e-02, -2.0862e-02, -1.2094e-02,  1.1698e-01, -4.5316e-02,\n",
       "          2.6681e-02,  2.2019e-02,  7.3707e-03,  4.1423e-02,  5.7961e-03,\n",
       "          3.9159e-02,  5.3507e-02,  1.3835e-02, -1.1384e-02,  1.4361e-01,\n",
       "          9.6650e-02, -2.9369e-02,  5.6706e-02,  4.0970e-02, -3.9212e-02,\n",
       "         -2.8348e-03,  2.0159e-02,  6.8199e-02,  2.6304e-02,  4.1895e-02,\n",
       "         -1.0610e-02,  7.5215e-02,  2.5240e-03,  1.6676e-02,  7.4663e-03,\n",
       "          1.4908e-02, -7.8240e-02,  4.1030e-02,  1.1917e-02, -1.7137e-02,\n",
       "          4.6536e-03, -6.1406e-02, -1.9746e-02,  6.5682e-04,  1.7550e-02,\n",
       "          5.9553e-02, -4.8032e-02,  8.1573e-02,  2.5199e-02,  6.5755e-04,\n",
       "         -1.4077e-02, -8.6827e-03,  1.1150e-01,  7.8599e-03,  3.6134e-02,\n",
       "         -1.5311e-02,  6.5817e-02,  1.3256e-02,  8.6665e-02, -8.3376e-03,\n",
       "          7.1361e-02, -4.2091e-03,  9.5070e-02,  6.6638e-03, -1.1876e-02,\n",
       "          6.9262e-02,  2.7988e-02, -2.7735e-02,  6.3962e-02, -4.1511e-02,\n",
       "         -2.1732e-02,  1.5812e-02,  1.7327e-03,  2.4281e-02,  1.8411e-02,\n",
       "          2.5790e-02,  8.1437e-02,  3.7063e-02, -5.0746e-02,  4.7191e-02,\n",
       "          5.8196e-04,  6.8993e-02, -6.5450e-03, -4.0825e-02,  7.4585e-03,\n",
       "         -6.2364e-02, -8.2996e-03,  8.9747e-02]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check that embeddings aren't the same\n",
    "embeds_dict['suicide'] - gen_embeds_dict['suicide']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c364fce2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>758</th>\n",
       "      <th>759</th>\n",
       "      <th>760</th>\n",
       "      <th>761</th>\n",
       "      <th>762</th>\n",
       "      <th>763</th>\n",
       "      <th>764</th>\n",
       "      <th>765</th>\n",
       "      <th>766</th>\n",
       "      <th>767</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.001036</td>\n",
       "      <td>0.000236</td>\n",
       "      <td>0.004341</td>\n",
       "      <td>0.000126</td>\n",
       "      <td>0.00722</td>\n",
       "      <td>0.002285</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.0035</td>\n",
       "      <td>0.001232</td>\n",
       "      <td>0.000417</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002875</td>\n",
       "      <td>0.002039</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004198</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.001818</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.004466</td>\n",
       "      <td>0.00007</td>\n",
       "      <td>0.006869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.001633</td>\n",
       "      <td>0.001055</td>\n",
       "      <td>0.001508</td>\n",
       "      <td>0.001337</td>\n",
       "      <td>0.000224</td>\n",
       "      <td>0.003158</td>\n",
       "      <td>0.000199</td>\n",
       "      <td>0.002035</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.004367</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004047</td>\n",
       "      <td>0.007014</td>\n",
       "      <td>0.000645</td>\n",
       "      <td>0.004294</td>\n",
       "      <td>0.010297</td>\n",
       "      <td>0.00025</td>\n",
       "      <td>0.000605</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.008244</td>\n",
       "      <td>0.002385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00062</td>\n",
       "      <td>0.000527</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>0.000748</td>\n",
       "      <td>0.00029</td>\n",
       "      <td>0.002701</td>\n",
       "      <td>0.002219</td>\n",
       "      <td>0.001808</td>\n",
       "      <td>0.005543</td>\n",
       "      <td>0.002104</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002543</td>\n",
       "      <td>0.000172</td>\n",
       "      <td>0.000603</td>\n",
       "      <td>0.005171</td>\n",
       "      <td>0.004048</td>\n",
       "      <td>0.000437</td>\n",
       "      <td>0.005253</td>\n",
       "      <td>0.000389</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000272</td>\n",
       "      <td>0.004882</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000323</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.001218</td>\n",
       "      <td>0.002926</td>\n",
       "      <td>0.004527</td>\n",
       "      <td>0.000088</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000414</td>\n",
       "      <td>0.001603</td>\n",
       "      <td>0.003136</td>\n",
       "      <td>0.002462</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.005404</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000706</td>\n",
       "      <td>0.016868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00229</td>\n",
       "      <td>0.000632</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000527</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.00489</td>\n",
       "      <td>0.001976</td>\n",
       "      <td>0.001975</td>\n",
       "      <td>0.002645</td>\n",
       "      <td>0.003032</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001456</td>\n",
       "      <td>0.001198</td>\n",
       "      <td>0.003191</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000529</td>\n",
       "      <td>0.000608</td>\n",
       "      <td>0.000212</td>\n",
       "      <td>0.005936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000146</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>0.001614</td>\n",
       "      <td>0.001939</td>\n",
       "      <td>0.000302</td>\n",
       "      <td>0.00527</td>\n",
       "      <td>0.002365</td>\n",
       "      <td>0.004933</td>\n",
       "      <td>0.003272</td>\n",
       "      <td>0.000152</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000861</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.001026</td>\n",
       "      <td>0.000847</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>0.000269</td>\n",
       "      <td>0.000775</td>\n",
       "      <td>0.002969</td>\n",
       "      <td>0.016121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.001562</td>\n",
       "      <td>0.004002</td>\n",
       "      <td>0.00639</td>\n",
       "      <td>0.00008</td>\n",
       "      <td>0.00621</td>\n",
       "      <td>0.00023</td>\n",
       "      <td>0.002496</td>\n",
       "      <td>0.000423</td>\n",
       "      <td>0.001179</td>\n",
       "      <td>0.001187</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000456</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.000788</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.000263</td>\n",
       "      <td>0.00004</td>\n",
       "      <td>0.000548</td>\n",
       "      <td>0.008951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.000148</td>\n",
       "      <td>0.000937</td>\n",
       "      <td>0.001131</td>\n",
       "      <td>0.002582</td>\n",
       "      <td>0.003639</td>\n",
       "      <td>0.002949</td>\n",
       "      <td>0.003304</td>\n",
       "      <td>0.00396</td>\n",
       "      <td>0.001022</td>\n",
       "      <td>0.004895</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001279</td>\n",
       "      <td>0.002511</td>\n",
       "      <td>0.004448</td>\n",
       "      <td>0.001061</td>\n",
       "      <td>0.001151</td>\n",
       "      <td>0.00009</td>\n",
       "      <td>0.000154</td>\n",
       "      <td>0.001443</td>\n",
       "      <td>0.003953</td>\n",
       "      <td>0.013421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.002309</td>\n",
       "      <td>0.000184</td>\n",
       "      <td>0.001338</td>\n",
       "      <td>0.000095</td>\n",
       "      <td>0.000564</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000121</td>\n",
       "      <td>0.000421</td>\n",
       "      <td>0.001694</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004716</td>\n",
       "      <td>0.001223</td>\n",
       "      <td>0.005008</td>\n",
       "      <td>0.001054</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000394</td>\n",
       "      <td>0.000588</td>\n",
       "      <td>0.000122</td>\n",
       "      <td>0.002374</td>\n",
       "      <td>0.002719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.000161</td>\n",
       "      <td>0.000093</td>\n",
       "      <td>0.006557</td>\n",
       "      <td>0.001191</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000192</td>\n",
       "      <td>0.002251</td>\n",
       "      <td>0.000175</td>\n",
       "      <td>0.000608</td>\n",
       "      <td>0.003551</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00142</td>\n",
       "      <td>0.000636</td>\n",
       "      <td>0.00042</td>\n",
       "      <td>0.003722</td>\n",
       "      <td>0.000087</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.000126</td>\n",
       "      <td>0.000436</td>\n",
       "      <td>0.000585</td>\n",
       "      <td>0.002599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.006041</td>\n",
       "      <td>0.000558</td>\n",
       "      <td>0.002866</td>\n",
       "      <td>0.001535</td>\n",
       "      <td>0.000846</td>\n",
       "      <td>0.007392</td>\n",
       "      <td>0.016011</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.004141</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003397</td>\n",
       "      <td>0.000246</td>\n",
       "      <td>0.005974</td>\n",
       "      <td>0.002033</td>\n",
       "      <td>0.005802</td>\n",
       "      <td>0.000911</td>\n",
       "      <td>0.000439</td>\n",
       "      <td>0.001075</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.005461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.001981</td>\n",
       "      <td>0.001798</td>\n",
       "      <td>0.001869</td>\n",
       "      <td>0.002801</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.00085</td>\n",
       "      <td>0.00235</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000169</td>\n",
       "      <td>0.00323</td>\n",
       "      <td>0.001842</td>\n",
       "      <td>0.000896</td>\n",
       "      <td>0.000106</td>\n",
       "      <td>0.000592</td>\n",
       "      <td>0.001649</td>\n",
       "      <td>0.000545</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>0.000392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.000762</td>\n",
       "      <td>0.000871</td>\n",
       "      <td>0.007792</td>\n",
       "      <td>0.001282</td>\n",
       "      <td>0.001156</td>\n",
       "      <td>0.006148</td>\n",
       "      <td>0.0043</td>\n",
       "      <td>0.00147</td>\n",
       "      <td>0.003431</td>\n",
       "      <td>0.001337</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001679</td>\n",
       "      <td>0.000329</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>0.000762</td>\n",
       "      <td>0.000104</td>\n",
       "      <td>0.000484</td>\n",
       "      <td>0.000292</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000151</td>\n",
       "      <td>0.003429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.000343</td>\n",
       "      <td>0.000434</td>\n",
       "      <td>0.01566</td>\n",
       "      <td>0.006138</td>\n",
       "      <td>0.001161</td>\n",
       "      <td>0.013789</td>\n",
       "      <td>0.006051</td>\n",
       "      <td>0.000351</td>\n",
       "      <td>0.000379</td>\n",
       "      <td>0.00225</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003909</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.001832</td>\n",
       "      <td>0.004131</td>\n",
       "      <td>0.000776</td>\n",
       "      <td>0.004303</td>\n",
       "      <td>0.000819</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000576</td>\n",
       "      <td>0.000722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.00024</td>\n",
       "      <td>0.000083</td>\n",
       "      <td>0.001525</td>\n",
       "      <td>0.000142</td>\n",
       "      <td>0.000846</td>\n",
       "      <td>0.007245</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.0031</td>\n",
       "      <td>0.003455</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000327</td>\n",
       "      <td>0.004986</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000633</td>\n",
       "      <td>0.000147</td>\n",
       "      <td>0.000422</td>\n",
       "      <td>0.00233</td>\n",
       "      <td>0.00161</td>\n",
       "      <td>0.002589</td>\n",
       "      <td>0.007886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.00262</td>\n",
       "      <td>0.002701</td>\n",
       "      <td>0.001981</td>\n",
       "      <td>0.001058</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.001414</td>\n",
       "      <td>0.002949</td>\n",
       "      <td>0.001854</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.003453</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000287</td>\n",
       "      <td>0.000607</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.004446</td>\n",
       "      <td>0.002057</td>\n",
       "      <td>0.001868</td>\n",
       "      <td>0.001728</td>\n",
       "      <td>0.000714</td>\n",
       "      <td>0.001419</td>\n",
       "      <td>0.005489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.003093</td>\n",
       "      <td>0.000588</td>\n",
       "      <td>0.001635</td>\n",
       "      <td>0.000134</td>\n",
       "      <td>0.003491</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000664</td>\n",
       "      <td>0.000279</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.000882</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000561</td>\n",
       "      <td>0.000108</td>\n",
       "      <td>0.000439</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000666</td>\n",
       "      <td>0.00172</td>\n",
       "      <td>0.000627</td>\n",
       "      <td>0.000522</td>\n",
       "      <td>0.0004</td>\n",
       "      <td>0.001569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.000484</td>\n",
       "      <td>0.000386</td>\n",
       "      <td>0.002019</td>\n",
       "      <td>0.001506</td>\n",
       "      <td>0.00057</td>\n",
       "      <td>0.000158</td>\n",
       "      <td>0.000875</td>\n",
       "      <td>0.004186</td>\n",
       "      <td>0.005249</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001982</td>\n",
       "      <td>0.00474</td>\n",
       "      <td>0.000795</td>\n",
       "      <td>0.00006</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000517</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.001647</td>\n",
       "      <td>0.002083</td>\n",
       "      <td>0.001772</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18 rows Ã— 768 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6    \\\n",
       "0   0.001036  0.000236  0.004341  0.000126   0.00722  0.002285  0.000056   \n",
       "1   0.001633  0.001055  0.001508  0.001337  0.000224  0.003158  0.000199   \n",
       "2    0.00062  0.000527  0.000062  0.000748   0.00029  0.002701  0.002219   \n",
       "3   0.000272  0.004882  0.000017       0.0  0.000323  0.000001  0.001218   \n",
       "4    0.00229  0.000632  0.000013  0.000527  0.000026   0.00489  0.001976   \n",
       "5   0.000146  0.000053  0.001614  0.001939  0.000302   0.00527  0.002365   \n",
       "6   0.001562  0.004002   0.00639   0.00008   0.00621   0.00023  0.002496   \n",
       "7   0.000148  0.000937  0.001131  0.002582  0.003639  0.002949  0.003304   \n",
       "8   0.002309  0.000184  0.001338  0.000095  0.000564  0.000001  0.000121   \n",
       "9   0.000161  0.000093  0.006557  0.001191  0.000034  0.000192  0.002251   \n",
       "10   0.00002  0.006041  0.000558  0.002866  0.001535  0.000846  0.007392   \n",
       "11  0.000026  0.001981  0.001798  0.001869  0.002801  0.000024  0.000081   \n",
       "12  0.000762  0.000871  0.007792  0.001282  0.001156  0.006148    0.0043   \n",
       "13  0.000343  0.000434   0.01566  0.006138  0.001161  0.013789  0.006051   \n",
       "14   0.00024  0.000083  0.001525  0.000142  0.000846  0.007245  0.000008   \n",
       "15   0.00262  0.002701  0.001981  0.001058   0.00002  0.001414  0.002949   \n",
       "16  0.003093  0.000588  0.001635  0.000134  0.003491  0.000008  0.000664   \n",
       "17  0.000484  0.000386  0.002019  0.001506   0.00057  0.000158  0.000875   \n",
       "\n",
       "         7         8         9    ...       758       759       760       761  \\\n",
       "0     0.0035  0.001232  0.000417  ...  0.002875  0.002039       0.0  0.004198   \n",
       "1   0.002035  0.000033  0.004367  ...  0.004047  0.007014  0.000645  0.004294   \n",
       "2   0.001808  0.005543  0.002104  ...  0.002543  0.000172  0.000603  0.005171   \n",
       "3   0.002926  0.004527  0.000088  ...  0.000037  0.000414  0.001603  0.003136   \n",
       "4   0.001975  0.002645  0.003032  ...  0.001456  0.001198  0.003191       0.0   \n",
       "5   0.004933  0.003272  0.000152  ...  0.000861  0.000016  0.001026  0.000847   \n",
       "6   0.000423  0.001179  0.001187  ...  0.000025  0.000456  0.000033  0.000788   \n",
       "7    0.00396  0.001022  0.004895  ...  0.001279  0.002511  0.004448  0.001061   \n",
       "8   0.000421  0.001694  0.000045  ...  0.004716  0.001223  0.005008  0.001054   \n",
       "9   0.000175  0.000608  0.003551  ...   0.00142  0.000636   0.00042  0.003722   \n",
       "10  0.016011   0.00001  0.004141  ...  0.003397  0.000246  0.005974  0.002033   \n",
       "11  0.000005   0.00085   0.00235  ...  0.000169   0.00323  0.001842  0.000896   \n",
       "12   0.00147  0.003431  0.001337  ...  0.001679  0.000329  0.000066  0.000762   \n",
       "13  0.000351  0.000379   0.00225  ...  0.003909  0.000002  0.001832  0.004131   \n",
       "14  0.000069    0.0031  0.003455  ...  0.000327  0.004986       0.0  0.000633   \n",
       "15  0.001854  0.000001  0.003453  ...  0.000287  0.000607  0.000002  0.004446   \n",
       "16  0.000279  0.000058  0.000882  ...  0.000561  0.000108  0.000439  0.000023   \n",
       "17  0.004186  0.005249  0.000001  ...  0.001982   0.00474  0.000795   0.00006   \n",
       "\n",
       "         762       763       764       765       766       767  \n",
       "0   0.000043  0.001818  0.000055  0.004466   0.00007  0.006869  \n",
       "1   0.010297   0.00025  0.000605  0.000022  0.008244  0.002385  \n",
       "2   0.004048  0.000437  0.005253  0.000389  0.000018  0.000889  \n",
       "3   0.002462  0.000046  0.005404  0.000007  0.000706  0.016868  \n",
       "4   0.000044  0.000028  0.000529  0.000608  0.000212  0.005936  \n",
       "5   0.000051   0.00005  0.000269  0.000775  0.002969  0.016121  \n",
       "6        0.0  0.000069  0.000263   0.00004  0.000548  0.008951  \n",
       "7   0.001151   0.00009  0.000154  0.001443  0.003953  0.013421  \n",
       "8   0.000001  0.000394  0.000588  0.000122  0.002374  0.002719  \n",
       "9   0.000087  0.000044  0.000126  0.000436  0.000585  0.002599  \n",
       "10  0.005802  0.000911  0.000439  0.001075  0.000007  0.005461  \n",
       "11  0.000106  0.000592  0.001649  0.000545  0.000144  0.000392  \n",
       "12  0.000104  0.000484  0.000292  0.000011  0.000151  0.003429  \n",
       "13  0.000776  0.004303  0.000819  0.000096  0.000576  0.000722  \n",
       "14  0.000147  0.000422   0.00233   0.00161  0.002589  0.007886  \n",
       "15  0.002057  0.001868  0.001728  0.000714  0.001419  0.005489  \n",
       "16  0.000666   0.00172  0.000627  0.000522    0.0004  0.001569  \n",
       "17  0.000029  0.000517  0.000011  0.001647  0.002083  0.001772  \n",
       "\n",
       "[18 rows x 768 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dataframe with new embeddings\n",
    "finetuned = pd.DataFrame(columns=range(len(list(embeds_dict['suicide'][0]))))\n",
    "for i, word in enumerate(list(suicide_dict.keys())):\n",
    "    finetuned.loc[i,:] = list(embeds_dict[word][0])\n",
    "\n",
    "finetuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "36714f07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>758</th>\n",
       "      <th>759</th>\n",
       "      <th>760</th>\n",
       "      <th>761</th>\n",
       "      <th>762</th>\n",
       "      <th>763</th>\n",
       "      <th>764</th>\n",
       "      <th>765</th>\n",
       "      <th>766</th>\n",
       "      <th>767</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.032195</td>\n",
       "      <td>0.015377</td>\n",
       "      <td>-0.065888</td>\n",
       "      <td>-0.011227</td>\n",
       "      <td>-0.084968</td>\n",
       "      <td>-0.047807</td>\n",
       "      <td>0.007506</td>\n",
       "      <td>-0.059158</td>\n",
       "      <td>0.035104</td>\n",
       "      <td>-0.020415</td>\n",
       "      <td>...</td>\n",
       "      <td>0.053621</td>\n",
       "      <td>-0.045152</td>\n",
       "      <td>-0.000582</td>\n",
       "      <td>-0.064795</td>\n",
       "      <td>0.006588</td>\n",
       "      <td>0.042643</td>\n",
       "      <td>-0.007404</td>\n",
       "      <td>0.066831</td>\n",
       "      <td>0.00837</td>\n",
       "      <td>-0.082878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.040416</td>\n",
       "      <td>-0.032474</td>\n",
       "      <td>-0.038835</td>\n",
       "      <td>-0.036569</td>\n",
       "      <td>0.014961</td>\n",
       "      <td>-0.056198</td>\n",
       "      <td>-0.014118</td>\n",
       "      <td>-0.045113</td>\n",
       "      <td>-0.005729</td>\n",
       "      <td>-0.066084</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.063616</td>\n",
       "      <td>-0.083751</td>\n",
       "      <td>0.0254</td>\n",
       "      <td>-0.06553</td>\n",
       "      <td>-0.101476</td>\n",
       "      <td>-0.015825</td>\n",
       "      <td>-0.024587</td>\n",
       "      <td>-0.004644</td>\n",
       "      <td>-0.090796</td>\n",
       "      <td>-0.048837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.024893</td>\n",
       "      <td>-0.022964</td>\n",
       "      <td>0.0079</td>\n",
       "      <td>-0.027356</td>\n",
       "      <td>-0.017038</td>\n",
       "      <td>-0.051974</td>\n",
       "      <td>-0.047102</td>\n",
       "      <td>-0.042519</td>\n",
       "      <td>-0.074454</td>\n",
       "      <td>-0.045871</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.050427</td>\n",
       "      <td>-0.013103</td>\n",
       "      <td>-0.024566</td>\n",
       "      <td>-0.071907</td>\n",
       "      <td>-0.063625</td>\n",
       "      <td>-0.020901</td>\n",
       "      <td>-0.072481</td>\n",
       "      <td>0.019714</td>\n",
       "      <td>-0.004297</td>\n",
       "      <td>-0.029812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.016499</td>\n",
       "      <td>-0.069873</td>\n",
       "      <td>0.004089</td>\n",
       "      <td>-0.000212</td>\n",
       "      <td>-0.017976</td>\n",
       "      <td>0.00105</td>\n",
       "      <td>-0.034903</td>\n",
       "      <td>-0.054091</td>\n",
       "      <td>-0.067285</td>\n",
       "      <td>0.009372</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006077</td>\n",
       "      <td>-0.020341</td>\n",
       "      <td>-0.040036</td>\n",
       "      <td>-0.056004</td>\n",
       "      <td>-0.04962</td>\n",
       "      <td>0.006775</td>\n",
       "      <td>-0.073514</td>\n",
       "      <td>0.002619</td>\n",
       "      <td>-0.026571</td>\n",
       "      <td>-0.129875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.047859</td>\n",
       "      <td>-0.025147</td>\n",
       "      <td>-0.003667</td>\n",
       "      <td>0.022963</td>\n",
       "      <td>0.005074</td>\n",
       "      <td>-0.06993</td>\n",
       "      <td>-0.044456</td>\n",
       "      <td>-0.044442</td>\n",
       "      <td>-0.051428</td>\n",
       "      <td>-0.055066</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.038152</td>\n",
       "      <td>-0.034611</td>\n",
       "      <td>-0.05649</td>\n",
       "      <td>0.000347</td>\n",
       "      <td>-0.006596</td>\n",
       "      <td>0.005326</td>\n",
       "      <td>-0.023009</td>\n",
       "      <td>-0.024648</td>\n",
       "      <td>-0.014543</td>\n",
       "      <td>-0.077043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.01207</td>\n",
       "      <td>-0.007247</td>\n",
       "      <td>-0.040171</td>\n",
       "      <td>-0.044033</td>\n",
       "      <td>-0.017387</td>\n",
       "      <td>-0.072592</td>\n",
       "      <td>-0.048629</td>\n",
       "      <td>-0.070234</td>\n",
       "      <td>-0.057202</td>\n",
       "      <td>-0.012341</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.029349</td>\n",
       "      <td>-0.003988</td>\n",
       "      <td>-0.032026</td>\n",
       "      <td>-0.029099</td>\n",
       "      <td>-0.007122</td>\n",
       "      <td>0.007039</td>\n",
       "      <td>0.016412</td>\n",
       "      <td>-0.027831</td>\n",
       "      <td>-0.054487</td>\n",
       "      <td>-0.126967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.039521</td>\n",
       "      <td>-0.063264</td>\n",
       "      <td>-0.079941</td>\n",
       "      <td>0.008971</td>\n",
       "      <td>0.078805</td>\n",
       "      <td>-0.015157</td>\n",
       "      <td>-0.049956</td>\n",
       "      <td>-0.020557</td>\n",
       "      <td>-0.034338</td>\n",
       "      <td>-0.034446</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.004999</td>\n",
       "      <td>-0.021351</td>\n",
       "      <td>0.005732</td>\n",
       "      <td>-0.028065</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>-0.008309</td>\n",
       "      <td>0.016225</td>\n",
       "      <td>-0.00635</td>\n",
       "      <td>-0.023418</td>\n",
       "      <td>-0.094609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.012147</td>\n",
       "      <td>-0.030616</td>\n",
       "      <td>-0.033624</td>\n",
       "      <td>-0.050812</td>\n",
       "      <td>0.060321</td>\n",
       "      <td>-0.054309</td>\n",
       "      <td>-0.057479</td>\n",
       "      <td>-0.062931</td>\n",
       "      <td>-0.031971</td>\n",
       "      <td>-0.069967</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.035764</td>\n",
       "      <td>-0.050114</td>\n",
       "      <td>0.066696</td>\n",
       "      <td>-0.032573</td>\n",
       "      <td>-0.033934</td>\n",
       "      <td>-0.009482</td>\n",
       "      <td>-0.01241</td>\n",
       "      <td>0.037991</td>\n",
       "      <td>-0.062871</td>\n",
       "      <td>-0.115848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.048053</td>\n",
       "      <td>0.013559</td>\n",
       "      <td>-0.036582</td>\n",
       "      <td>-0.009722</td>\n",
       "      <td>0.023755</td>\n",
       "      <td>-0.001023</td>\n",
       "      <td>-0.010981</td>\n",
       "      <td>-0.020512</td>\n",
       "      <td>0.041153</td>\n",
       "      <td>-0.006717</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.068673</td>\n",
       "      <td>-0.03497</td>\n",
       "      <td>-0.070767</td>\n",
       "      <td>-0.032472</td>\n",
       "      <td>0.000973</td>\n",
       "      <td>0.019857</td>\n",
       "      <td>-0.024249</td>\n",
       "      <td>0.011052</td>\n",
       "      <td>-0.048721</td>\n",
       "      <td>-0.052141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.012672</td>\n",
       "      <td>-0.009662</td>\n",
       "      <td>-0.080972</td>\n",
       "      <td>-0.034515</td>\n",
       "      <td>-0.005849</td>\n",
       "      <td>0.013862</td>\n",
       "      <td>0.047449</td>\n",
       "      <td>0.013215</td>\n",
       "      <td>0.02466</td>\n",
       "      <td>-0.059589</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.037685</td>\n",
       "      <td>-0.025229</td>\n",
       "      <td>-0.020488</td>\n",
       "      <td>-0.06101</td>\n",
       "      <td>-0.009308</td>\n",
       "      <td>-0.006612</td>\n",
       "      <td>-0.011241</td>\n",
       "      <td>-0.02089</td>\n",
       "      <td>0.02419</td>\n",
       "      <td>-0.050977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.004492</td>\n",
       "      <td>-0.077725</td>\n",
       "      <td>-0.02362</td>\n",
       "      <td>-0.053539</td>\n",
       "      <td>0.039179</td>\n",
       "      <td>-0.029085</td>\n",
       "      <td>-0.085977</td>\n",
       "      <td>-0.126535</td>\n",
       "      <td>0.003234</td>\n",
       "      <td>-0.064349</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.058288</td>\n",
       "      <td>-0.015685</td>\n",
       "      <td>-0.077292</td>\n",
       "      <td>-0.04509</td>\n",
       "      <td>-0.076168</td>\n",
       "      <td>-0.030178</td>\n",
       "      <td>-0.020964</td>\n",
       "      <td>-0.032783</td>\n",
       "      <td>-0.002562</td>\n",
       "      <td>-0.073897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.005113</td>\n",
       "      <td>-0.04451</td>\n",
       "      <td>0.042403</td>\n",
       "      <td>0.043237</td>\n",
       "      <td>-0.052921</td>\n",
       "      <td>0.004889</td>\n",
       "      <td>-0.009002</td>\n",
       "      <td>0.002274</td>\n",
       "      <td>-0.029158</td>\n",
       "      <td>-0.048481</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.013008</td>\n",
       "      <td>-0.056836</td>\n",
       "      <td>0.042918</td>\n",
       "      <td>-0.029929</td>\n",
       "      <td>0.010314</td>\n",
       "      <td>-0.024335</td>\n",
       "      <td>0.040611</td>\n",
       "      <td>-0.023342</td>\n",
       "      <td>-0.012013</td>\n",
       "      <td>0.019796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.027604</td>\n",
       "      <td>-0.029507</td>\n",
       "      <td>-0.088272</td>\n",
       "      <td>-0.035806</td>\n",
       "      <td>-0.034003</td>\n",
       "      <td>-0.078411</td>\n",
       "      <td>-0.065577</td>\n",
       "      <td>-0.038335</td>\n",
       "      <td>-0.058578</td>\n",
       "      <td>-0.036568</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.040974</td>\n",
       "      <td>0.018128</td>\n",
       "      <td>-0.008104</td>\n",
       "      <td>-0.027613</td>\n",
       "      <td>-0.010216</td>\n",
       "      <td>-0.022001</td>\n",
       "      <td>-0.017088</td>\n",
       "      <td>0.003386</td>\n",
       "      <td>-0.012275</td>\n",
       "      <td>-0.058562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.018517</td>\n",
       "      <td>-0.020842</td>\n",
       "      <td>-0.125138</td>\n",
       "      <td>-0.078343</td>\n",
       "      <td>0.034067</td>\n",
       "      <td>-0.117427</td>\n",
       "      <td>-0.077789</td>\n",
       "      <td>-0.018746</td>\n",
       "      <td>-0.019464</td>\n",
       "      <td>-0.047438</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.06252</td>\n",
       "      <td>0.001261</td>\n",
       "      <td>-0.042803</td>\n",
       "      <td>-0.064271</td>\n",
       "      <td>-0.027864</td>\n",
       "      <td>-0.065595</td>\n",
       "      <td>0.028627</td>\n",
       "      <td>0.009808</td>\n",
       "      <td>-0.024003</td>\n",
       "      <td>-0.026863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.015502</td>\n",
       "      <td>-0.009109</td>\n",
       "      <td>-0.039055</td>\n",
       "      <td>-0.011899</td>\n",
       "      <td>-0.029082</td>\n",
       "      <td>-0.085117</td>\n",
       "      <td>-0.002865</td>\n",
       "      <td>0.00828</td>\n",
       "      <td>-0.055681</td>\n",
       "      <td>-0.058777</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018088</td>\n",
       "      <td>-0.070615</td>\n",
       "      <td>-0.000151</td>\n",
       "      <td>-0.025162</td>\n",
       "      <td>-0.012138</td>\n",
       "      <td>0.020553</td>\n",
       "      <td>0.048273</td>\n",
       "      <td>-0.040129</td>\n",
       "      <td>-0.050882</td>\n",
       "      <td>-0.0888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-0.051182</td>\n",
       "      <td>-0.051968</td>\n",
       "      <td>-0.044508</td>\n",
       "      <td>-0.032522</td>\n",
       "      <td>-0.004518</td>\n",
       "      <td>-0.037599</td>\n",
       "      <td>-0.054309</td>\n",
       "      <td>-0.043056</td>\n",
       "      <td>-0.000728</td>\n",
       "      <td>-0.05876</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.01693</td>\n",
       "      <td>0.024643</td>\n",
       "      <td>-0.001364</td>\n",
       "      <td>-0.066676</td>\n",
       "      <td>-0.045357</td>\n",
       "      <td>-0.04322</td>\n",
       "      <td>-0.041565</td>\n",
       "      <td>-0.026719</td>\n",
       "      <td>-0.037664</td>\n",
       "      <td>-0.074085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-0.055617</td>\n",
       "      <td>-0.024248</td>\n",
       "      <td>-0.040431</td>\n",
       "      <td>-0.011575</td>\n",
       "      <td>0.059089</td>\n",
       "      <td>0.002784</td>\n",
       "      <td>-0.025776</td>\n",
       "      <td>-0.01669</td>\n",
       "      <td>0.007606</td>\n",
       "      <td>-0.029704</td>\n",
       "      <td>...</td>\n",
       "      <td>0.02369</td>\n",
       "      <td>0.01037</td>\n",
       "      <td>-0.020958</td>\n",
       "      <td>0.004768</td>\n",
       "      <td>0.0258</td>\n",
       "      <td>-0.04147</td>\n",
       "      <td>-0.025035</td>\n",
       "      <td>0.022837</td>\n",
       "      <td>-0.019993</td>\n",
       "      <td>-0.03961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-0.022005</td>\n",
       "      <td>-0.019648</td>\n",
       "      <td>-0.044932</td>\n",
       "      <td>-0.038813</td>\n",
       "      <td>-0.023872</td>\n",
       "      <td>-0.012574</td>\n",
       "      <td>0.029583</td>\n",
       "      <td>-0.064699</td>\n",
       "      <td>-0.072448</td>\n",
       "      <td>-0.000898</td>\n",
       "      <td>...</td>\n",
       "      <td>0.044517</td>\n",
       "      <td>-0.068846</td>\n",
       "      <td>-0.028202</td>\n",
       "      <td>0.007714</td>\n",
       "      <td>-0.00534</td>\n",
       "      <td>0.022743</td>\n",
       "      <td>-0.003377</td>\n",
       "      <td>0.040578</td>\n",
       "      <td>0.045635</td>\n",
       "      <td>-0.042097</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18 rows Ã— 768 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6    \\\n",
       "0  -0.032195  0.015377 -0.065888 -0.011227 -0.084968 -0.047807  0.007506   \n",
       "1  -0.040416 -0.032474 -0.038835 -0.036569  0.014961 -0.056198 -0.014118   \n",
       "2  -0.024893 -0.022964    0.0079 -0.027356 -0.017038 -0.051974 -0.047102   \n",
       "3  -0.016499 -0.069873  0.004089 -0.000212 -0.017976   0.00105 -0.034903   \n",
       "4  -0.047859 -0.025147 -0.003667  0.022963  0.005074  -0.06993 -0.044456   \n",
       "5    0.01207 -0.007247 -0.040171 -0.044033 -0.017387 -0.072592 -0.048629   \n",
       "6  -0.039521 -0.063264 -0.079941  0.008971  0.078805 -0.015157 -0.049956   \n",
       "7   0.012147 -0.030616 -0.033624 -0.050812  0.060321 -0.054309 -0.057479   \n",
       "8  -0.048053  0.013559 -0.036582 -0.009722  0.023755 -0.001023 -0.010981   \n",
       "9   0.012672 -0.009662 -0.080972 -0.034515 -0.005849  0.013862  0.047449   \n",
       "10  0.004492 -0.077725  -0.02362 -0.053539  0.039179 -0.029085 -0.085977   \n",
       "11  0.005113  -0.04451  0.042403  0.043237 -0.052921  0.004889 -0.009002   \n",
       "12  0.027604 -0.029507 -0.088272 -0.035806 -0.034003 -0.078411 -0.065577   \n",
       "13 -0.018517 -0.020842 -0.125138 -0.078343  0.034067 -0.117427 -0.077789   \n",
       "14 -0.015502 -0.009109 -0.039055 -0.011899 -0.029082 -0.085117 -0.002865   \n",
       "15 -0.051182 -0.051968 -0.044508 -0.032522 -0.004518 -0.037599 -0.054309   \n",
       "16 -0.055617 -0.024248 -0.040431 -0.011575  0.059089  0.002784 -0.025776   \n",
       "17 -0.022005 -0.019648 -0.044932 -0.038813 -0.023872 -0.012574  0.029583   \n",
       "\n",
       "         7         8         9    ...       758       759       760       761  \\\n",
       "0  -0.059158  0.035104 -0.020415  ...  0.053621 -0.045152 -0.000582 -0.064795   \n",
       "1  -0.045113 -0.005729 -0.066084  ... -0.063616 -0.083751    0.0254  -0.06553   \n",
       "2  -0.042519 -0.074454 -0.045871  ... -0.050427 -0.013103 -0.024566 -0.071907   \n",
       "3  -0.054091 -0.067285  0.009372  ...  0.006077 -0.020341 -0.040036 -0.056004   \n",
       "4  -0.044442 -0.051428 -0.055066  ... -0.038152 -0.034611  -0.05649  0.000347   \n",
       "5  -0.070234 -0.057202 -0.012341  ... -0.029349 -0.003988 -0.032026 -0.029099   \n",
       "6  -0.020557 -0.034338 -0.034446  ... -0.004999 -0.021351  0.005732 -0.028065   \n",
       "7  -0.062931 -0.031971 -0.069967  ... -0.035764 -0.050114  0.066696 -0.032573   \n",
       "8  -0.020512  0.041153 -0.006717  ... -0.068673  -0.03497 -0.070767 -0.032472   \n",
       "9   0.013215   0.02466 -0.059589  ... -0.037685 -0.025229 -0.020488  -0.06101   \n",
       "10 -0.126535  0.003234 -0.064349  ... -0.058288 -0.015685 -0.077292  -0.04509   \n",
       "11  0.002274 -0.029158 -0.048481  ... -0.013008 -0.056836  0.042918 -0.029929   \n",
       "12 -0.038335 -0.058578 -0.036568  ... -0.040974  0.018128 -0.008104 -0.027613   \n",
       "13 -0.018746 -0.019464 -0.047438  ...  -0.06252  0.001261 -0.042803 -0.064271   \n",
       "14   0.00828 -0.055681 -0.058777  ...  0.018088 -0.070615 -0.000151 -0.025162   \n",
       "15 -0.043056 -0.000728  -0.05876  ...  -0.01693  0.024643 -0.001364 -0.066676   \n",
       "16  -0.01669  0.007606 -0.029704  ...   0.02369   0.01037 -0.020958  0.004768   \n",
       "17 -0.064699 -0.072448 -0.000898  ...  0.044517 -0.068846 -0.028202  0.007714   \n",
       "\n",
       "         762       763       764       765       766       767  \n",
       "0   0.006588  0.042643 -0.007404  0.066831   0.00837 -0.082878  \n",
       "1  -0.101476 -0.015825 -0.024587 -0.004644 -0.090796 -0.048837  \n",
       "2  -0.063625 -0.020901 -0.072481  0.019714 -0.004297 -0.029812  \n",
       "3   -0.04962  0.006775 -0.073514  0.002619 -0.026571 -0.129875  \n",
       "4  -0.006596  0.005326 -0.023009 -0.024648 -0.014543 -0.077043  \n",
       "5  -0.007122  0.007039  0.016412 -0.027831 -0.054487 -0.126967  \n",
       "6   0.000051 -0.008309  0.016225  -0.00635 -0.023418 -0.094609  \n",
       "7  -0.033934 -0.009482  -0.01241  0.037991 -0.062871 -0.115848  \n",
       "8   0.000973  0.019857 -0.024249  0.011052 -0.048721 -0.052141  \n",
       "9  -0.009308 -0.006612 -0.011241  -0.02089   0.02419 -0.050977  \n",
       "10 -0.076168 -0.030178 -0.020964 -0.032783 -0.002562 -0.073897  \n",
       "11  0.010314 -0.024335  0.040611 -0.023342 -0.012013  0.019796  \n",
       "12 -0.010216 -0.022001 -0.017088  0.003386 -0.012275 -0.058562  \n",
       "13 -0.027864 -0.065595  0.028627  0.009808 -0.024003 -0.026863  \n",
       "14 -0.012138  0.020553  0.048273 -0.040129 -0.050882   -0.0888  \n",
       "15 -0.045357  -0.04322 -0.041565 -0.026719 -0.037664 -0.074085  \n",
       "16    0.0258  -0.04147 -0.025035  0.022837 -0.019993  -0.03961  \n",
       "17  -0.00534  0.022743 -0.003377  0.040578  0.045635 -0.042097  \n",
       "\n",
       "[18 rows x 768 columns]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dataframe with old embeddings\n",
    "generic = pd.DataFrame(columns=range(len(list(gen_embeds_dict['suicide'][0]))))\n",
    "for i, word in enumerate(list(suicide_dict.keys())):\n",
    "    generic.loc[i,:] = list(gen_embeds_dict[word][0])\n",
    "\n",
    "generic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "801fe68a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pc1</th>\n",
       "      <th>pc2</th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.011878</td>\n",
       "      <td>-0.004737</td>\n",
       "      <td>suicide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.022647</td>\n",
       "      <td>0.015876</td>\n",
       "      <td>courage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.014738</td>\n",
       "      <td>0.006162</td>\n",
       "      <td>guilt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.015615</td>\n",
       "      <td>0.002057</td>\n",
       "      <td>shame</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.000131</td>\n",
       "      <td>-0.003212</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.024431</td>\n",
       "      <td>-0.037805</td>\n",
       "      <td>ecstasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.002295</td>\n",
       "      <td>-0.022583</td>\n",
       "      <td>despair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.005079</td>\n",
       "      <td>-0.033537</td>\n",
       "      <td>misery</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.021701</td>\n",
       "      <td>0.002311</td>\n",
       "      <td>romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.019892</td>\n",
       "      <td>0.018024</td>\n",
       "      <td>agency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-0.008248</td>\n",
       "      <td>0.012459</td>\n",
       "      <td>nobility</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.008990</td>\n",
       "      <td>0.048279</td>\n",
       "      <td>meaning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.070432</td>\n",
       "      <td>0.005425</td>\n",
       "      <td>meaningless</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.050352</td>\n",
       "      <td>0.036294</td>\n",
       "      <td>meaningful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.005976</td>\n",
       "      <td>-0.008419</td>\n",
       "      <td>punishment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.010576</td>\n",
       "      <td>-0.012830</td>\n",
       "      <td>hopeless</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-0.018690</td>\n",
       "      <td>-0.011044</td>\n",
       "      <td>hope</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-0.014657</td>\n",
       "      <td>-0.012718</td>\n",
       "      <td>escape</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         pc1       pc2         word\n",
       "0  -0.011878 -0.004737      suicide\n",
       "1  -0.022647  0.015876      courage\n",
       "2  -0.014738  0.006162        guilt\n",
       "3  -0.015615  0.002057        shame\n",
       "4  -0.000131 -0.003212          joy\n",
       "5   0.024431 -0.037805      ecstasy\n",
       "6   0.002295 -0.022583      despair\n",
       "7   0.005079 -0.033537       misery\n",
       "8  -0.021701  0.002311      romance\n",
       "9  -0.019892  0.018024       agency\n",
       "10 -0.008248  0.012459     nobility\n",
       "11 -0.008990  0.048279      meaning\n",
       "12  0.070432  0.005425  meaningless\n",
       "13  0.050352  0.036294   meaningful\n",
       "14 -0.005976 -0.008419   punishment\n",
       "15  0.010576 -0.012830     hopeless\n",
       "16 -0.018690 -0.011044         hope\n",
       "17 -0.014657 -0.012718       escape"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#run pca on finetuned embeddings\n",
    "pca = PCA(n_components=2)\n",
    "f_principalComponents = pca.fit_transform(finetuned)\n",
    "f_principalDf = pd.DataFrame(data = f_principalComponents, columns = ['pc1', 'pc2'])\n",
    "f_principalDf[\"word\"] = list(suicide_dict.keys())\n",
    "f_principalDf.to_csv('finetuned.csv')\n",
    "f_principalDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a08f8ab2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pc1</th>\n",
       "      <th>pc2</th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.014640</td>\n",
       "      <td>-0.442381</td>\n",
       "      <td>suicide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.250013</td>\n",
       "      <td>-0.040207</td>\n",
       "      <td>courage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.034430</td>\n",
       "      <td>-0.124596</td>\n",
       "      <td>guilt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.001847</td>\n",
       "      <td>-0.124881</td>\n",
       "      <td>shame</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.183386</td>\n",
       "      <td>0.027150</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.233327</td>\n",
       "      <td>-0.127098</td>\n",
       "      <td>ecstasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.167299</td>\n",
       "      <td>-0.216722</td>\n",
       "      <td>despair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.237286</td>\n",
       "      <td>-0.101692</td>\n",
       "      <td>misery</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.142678</td>\n",
       "      <td>-0.156663</td>\n",
       "      <td>romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.319359</td>\n",
       "      <td>0.204361</td>\n",
       "      <td>agency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.074606</td>\n",
       "      <td>0.062168</td>\n",
       "      <td>nobility</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.375735</td>\n",
       "      <td>0.820044</td>\n",
       "      <td>meaning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.673361</td>\n",
       "      <td>0.249775</td>\n",
       "      <td>meaningless</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.478335</td>\n",
       "      <td>0.440727</td>\n",
       "      <td>meaningful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.041631</td>\n",
       "      <td>-0.048996</td>\n",
       "      <td>punishment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.327791</td>\n",
       "      <td>-0.143353</td>\n",
       "      <td>hopeless</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-0.493599</td>\n",
       "      <td>0.041597</td>\n",
       "      <td>hope</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-0.334687</td>\n",
       "      <td>-0.319234</td>\n",
       "      <td>escape</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         pc1       pc2         word\n",
       "0  -0.014640 -0.442381      suicide\n",
       "1  -0.250013 -0.040207      courage\n",
       "2  -0.034430 -0.124596        guilt\n",
       "3  -0.001847 -0.124881        shame\n",
       "4  -0.183386  0.027150          joy\n",
       "5   0.233327 -0.127098      ecstasy\n",
       "6   0.167299 -0.216722      despair\n",
       "7   0.237286 -0.101692       misery\n",
       "8  -0.142678 -0.156663      romance\n",
       "9  -0.319359  0.204361       agency\n",
       "10  0.074606  0.062168     nobility\n",
       "11 -0.375735  0.820044      meaning\n",
       "12  0.673361  0.249775  meaningless\n",
       "13  0.478335  0.440727   meaningful\n",
       "14 -0.041631 -0.048996   punishment\n",
       "15  0.327791 -0.143353     hopeless\n",
       "16 -0.493599  0.041597         hope\n",
       "17 -0.334687 -0.319234       escape"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#run pca on old embeddings\n",
    "g_principalComponents = pca.fit_transform(generic)\n",
    "g_principalDf = pd.DataFrame(data = g_principalComponents, columns = ['pc1', 'pc2'])\n",
    "g_principalDf['word'] = list(suicide_dict.keys())\n",
    "g_principalDf\n",
    "\n",
    "g_principalDf.to_csv('generic.csv')\n",
    "\n",
    "g_principalDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0211fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#empty dataframes for cosine similarity matrices\n",
    "gen_sim = pd.DataFrame(columns=list(suicide_dict.keys()), index=list(suicide_dict.keys()))\n",
    "fin_sim = pd.DataFrame(columns=list(suicide_dict.keys()), index=list(suicide_dict.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6e5811",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fill matrices w cosine similarity\n",
    "for i, word in enumerate(list(suicide_dict.keys())):\n",
    "    for j, w in enumerate(list(suicide_dict.keys())):\n",
    "        gen_sim.iloc[i,j] = np.dot(generic.iloc[i,:], generic.iloc[j,:])/(np.linalg.norm(generic.iloc[i,:])*np.linalg.norm(generic.iloc[j,:]))\n",
    "        fin_sim.iloc[i,j] = np.dot(finetuned.iloc[i,:], finetuned.iloc[j,:])/(np.linalg.norm(finetuned.iloc[i,:])*np.linalg.norm(finetuned.iloc[j,:]))\n",
    "\n",
    "    \n",
    "gen_sim.to_csv('gen_sim.csv')\n",
    "fin_sim.to_csv('fin_sim.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
